# NLP_Yelp_Reviews

In this work, a Transformer model, DistilBERT, is compared against two baselines, a Naïve Bayes classifier, and a linear Support Vector Machine, for the task of classifying Yelp Review stars. The DistilBERT model was already pre-trained, but then fine-tuned on the Yelp Open Dataset which contains almost 7 million reviews. The model was fine-tuned for a total of 6.5 hours, and could likely have improved more given more time, but this was not possible due to computational limitations. The models were compared using accuracy and macro F1-score, where DistilBERT achieved the best results with an accuracy of 0.76 and an F1-score of 0.68. The SVM attained an accuracy of 0.67 and an F1-score of 0.52, while the Naïve Bayes classifier achieved an accuracy of 0.61 and an F1-score of 0.51.
